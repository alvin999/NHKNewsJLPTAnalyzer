import json
import os
import time
from backend.crawl import fetch_nhk_news, fetch_article_full_text

# æ ¹æ“šç’°å¢ƒæ±ºå®šè³‡æ–™åº«è·¯å¾‘
if os.getenv("GITHUB_ACTIONS"):
    DB_PATH = "data/news_db.json"
else:
    DB_PATH = "data/news_db_test.json"

def run_sync():
    print(f"ğŸ“¡ [{time.strftime('%H:%M:%S')}] é–‹å§‹åŒæ­¥ NHK æ–°è...")
    os.makedirs(os.path.dirname(DB_PATH), exist_ok=True)

    # 1. æŠ“å–æœ€æ–°æ¸…å–®
    df_list = fetch_nhk_news()
    if df_list.empty:
        print("âŒ ç„¡æ³•å–å¾—æ–°èæ¸…å–®ã€‚")
        return

    # 2. è®€å–ç¾æœ‰è³‡æ–™
    db = {}
    if os.path.exists(DB_PATH):
        try:
            with open(DB_PATH, 'r', encoding='utf-8') as f:
                db = json.load(f)
        except:
            db = {}

    # 3. å¢é‡çˆ¬å–æ–°æ–‡ç« 
    new_count = 0
    for _, row in df_list.iterrows():
        aid = str(row['id'])
        if aid not in db:
            print(f"ğŸ” çˆ¬å–æ–°æ–°è: {row['title']}")
            content = fetch_article_full_text(row['url'])
            if content:
                db[aid] = {
                    "title": row['title'],
                    "url": row['url'],
                    "content": content,
                    "timestamp": time.time()
                }
                new_count += 1
                time.sleep(1) # å‹å–„çˆ¬èŸ²å»¶é²

    # 4. è‡ªå‹•è¦†è“‹ï¼šæŒ‰æ™‚é–“æ’åºä¸¦ä¿ç•™æœ€æ–° 15 å‰‡
    sorted_items = sorted(db.items(), key=lambda x: x[1]['timestamp'], reverse=True)[:15]
    final_db = dict(sorted_items)

    # 5. å¯«å›è³‡æ–™åº«
    with open(DB_PATH, 'w', encoding='utf-8') as f:
        json.dump(final_db, f, ensure_ascii=False, indent=4)

    print(f"âœ… åŒæ­¥å®Œæˆï¼æ–°å¢ {new_count} å‰‡ï¼Œç›®å‰å…± {len(final_db)} å‰‡å„²å­˜æ–¼è³‡æ–™åº«ã€‚")

if __name__ == "__main__":
    run_sync()