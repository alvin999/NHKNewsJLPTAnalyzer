import requests
import pandas as pd
import os
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
#from playwright_stealth import stealth

def fetch_nhk_news():
    api_url = "https://www3.nhk.or.jp/news/json16/new_001.json"
    headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36'}
    try:
        response = requests.get(api_url, headers=headers, timeout=10)
        data = response.json()
        items = data.get('channel', {}).get('item', [])
        
        articles = []
        for item in items:
            this_id = item.get('id')
            # å¾ API çš„ link æå–åƒ k10015029391000 é€™æ¨£çš„å®Œæ•´ ID
            # ç¯„ä¾‹ link: "html/20260118/k10015029511000.html"
            raw_link = item.get('link', '')
            full_id_with_prefix = raw_link.split('/')[-1].replace('.html', '')
            
            # ç”¢ç”Ÿ NA ç¶²å€
            na_url = f"https://news.web.nhk/newsweb/na/na-{full_id_with_prefix}"
            
            articles.append({
                'id': this_id,
                'title': item.get('title'),
                'url': na_url
            })
        return pd.DataFrame(articles)
    except Exception as e:
        print(f"âŒ æŠ“å–æ¸…å–®å¤±æ•—: {e}")
        return pd.DataFrame()

    except Exception as e:
        print(f"âŒ æŠ“å–å¤±æ•—: {e}")
        return pd.DataFrame()

def fetch_article_full_text(url):
    paragraphs = [] # é è¨­ç©ºåˆ—è¡¨ï¼Œé¿å…å¤±æ•—æ™‚å›å‚³ None
    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(viewport={'width': 1280, 'height': 1200})
            page = context.new_page()
            
            # å‰å¾€ç¶²å€
            page.goto(url, wait_until="domcontentloaded")

            # --- æ ¸å¿ƒï¼šæ“Šç©¿æµ·å¤–å½ˆçª— ---
            try:
                # ä½¿ç”¨ä½ æä¾›çš„ç‰¹å®šæŒ‰éˆ•ç‰¹å¾µ
                confirm_selector = "button:has-text('ç¢ºèªã—ã¾ã—ãŸ')"
                page.wait_for_selector(confirm_selector, timeout=5000)
                page.click(confirm_selector)
                print("âœ… æˆåŠŸé»æ“Šã€ç¢ºèªã—ã¾ã—ãŸã€æŒ‰éˆ•")
                page.wait_for_timeout(1500)
            except Exception as e:
                print(f"â„¹ï¸ æœªç™¼ç¾å½ˆçª—æˆ–æŒ‰éˆ•å·²å¤±æ•ˆ: {e}")

            # --- æ¨¡æ“¬æ²å‹•è§¸ç™¼ Lazy Loading ---
            #page.mouse.wheel(0, 1500)
            #page.wait_for_timeout(2000)
            # --- æ ¸å¿ƒç°¡åŒ–ï¼šå°‡ HTML è½‰äº¤çµ¦ BeautifulSoup ---
            html_content = page.content()
            
            browser.close()
            
            soup = BeautifulSoup(html_content, 'html.parser')
            
            # 4. ä½¿ç”¨ BeautifulSoup å°‹æ‰¾æ‰€æœ‰å…§æ–‡æ¨™ç±¤
            nodes = soup.find_all(['p', 'h3'], class_=['_1i1d7sh2', '_1i1d7sh9'])
            
            # æå–æ–‡å­—ä¸¦éæ¿¾ç©ºå€¼
            paragraphs = [n.get_text().strip() for n in nodes if n.get_text().strip()]

            return paragraphs

    except Exception as e:
        print(f"âŒ Playwright éç¨‹å‡ºéŒ¯: {e}")
        return [f"æŠ“å–å¤±æ•—: {str(e)}"]


# --- æ¸¬è©¦èˆ‡åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    print("ğŸ“¡ æ­£åœ¨å˜—è©¦æŠ“å– NHK æœ€æ–°æ–°è...")
    
    df = fetch_nhk_news_list()
    
    if not df.empty:
        os.makedirs('data', exist_ok=True)
        # å„²å­˜æ¸…å–®
        df.to_csv('data/latest_articles.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸï¼æŠ“å–åˆ° {len(df)} å‰‡æ–°èæ¸…å–®ã€‚")
        
        # æ¸¬è©¦æŠ“å–ç¬¬ä¸€å‰‡çš„å…¨æ–‡
        first_url = df.iloc[0]['url']
        print(f"ğŸ” æ¸¬è©¦æŠ“å–ç¬¬ä¸€å‰‡å…¨æ–‡: {first_url}")
        content = fetch_article_full_text(first_url)
        print(f"ğŸ“ å…§æ–‡æ®µè½æ•¸: {len(content)}")
        for i, p in enumerate(content[:3]): # å°å‡ºå‰ä¸‰æ®µçœ‹çœ‹
            print(f"  æ®µè½ {i+1}: {p[:50]}...")
    else:
        print("âŒ ä¾ç„¶ç„¡æ³•æŠ“å–è³‡æ–™ï¼Œè«‹æª¢æŸ¥ JSON çµæ§‹ã€‚")