import requests
import pandas as pd
import os
from bs4 import BeautifulSoup
from playwright.sync_api import sync_playwright
import urllib.request
import json
#from playwright_stealth import stealth

def fetch_nhk_news():
    api_url = "https://www3.nhk.or.jp/news/json16/new_001.json"
    
    # ä½¿ç”¨æœ€æ¥µç°¡ä½†æœ‰æ•ˆçš„ Headers
    headers = {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Chrome/121.0.0.0 Safari/537.36',
        'Accept': 'application/json'
    }
    
    try:
        req = urllib.request.Request(api_url, headers=headers)
        # é€™è£¡ä¸ä½¿ç”¨ HTTP/2ï¼Œç›´æ¥èµ°æ¨™æº– HTTP/1.1 ä¸²æµ
        with urllib.request.urlopen(req, timeout=15) as response:
            if response.status == 200:
                data = json.loads(response.read().decode('utf-8'))
                items = data.get('channel', {}).get('item', [])
                
                articles = []
                for item in items:
                    raw_link = item.get('link', '')
                    full_id = raw_link.split('/')[-1].replace('.html', '')
                    na_url = f"https://news.web.nhk/newsweb/na/na-{full_id}"
                    
                    articles.append({
                        'id': item.get('id'),
                        'title': item.get('title'),
                        'url': na_url
                    })
                return pd.DataFrame(articles)
            else:
                print(f"âŒ ä¼ºæœå™¨å›å‚³ç‹€æ…‹ç¢¼: {response.status}")
                return pd.DataFrame()
                
    except Exception as e:
        print(f"âŒ urllib æŠ“å–å¤±æ•—: {e}")
        return pd.DataFrame()

def fetch_article_full_text(url):
    paragraphs = []
    # å»ºç«‹é™¤éŒ¯æˆªåœ–è³‡æ–™å¤¾
    debug_dir = "debug_steps"
    if not os.path.exists(debug_dir):
        os.makedirs(debug_dir)
    try:
        with sync_playwright() as p:
            # 1. æ¨¡æ“¬ Codegen çš„å•Ÿå‹•ç’°å¢ƒ
            browser = p.chromium.launch(headless=True)
            context = browser.new_context(
                viewport={'width': 1280, 'height': 800},
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36"
            )
            page = context.new_page()
            
            # 2. å‰å¾€ç¶²å€
            page.goto(url, wait_until="domcontentloaded")

            # --- é—œéµä¿®æ­£ï¼šåƒ Codegen ä¸€æ¨£éˆæ´»æ‡‰å° ---
            # æœ‰äº›ç’°å¢ƒæœƒè·³å‡º 1/2 å±¤ï¼Œæœ‰äº›ç›´æ¥è·³ 3 å±¤ã€‚æˆ‘å€‘ç”¨ try-except åŒ…èµ·ä¾†ã€‚
            
            # å˜—è©¦æ“Šç©¿ç¬¬ 1 & 2 å±¤ (å¦‚æœæœ‰çš„è©±)
            try:
                if page.get_by_text("å†…å®¹ã«ã¤ã„ã¦ç¢ºèªã—ã¾ã—ãŸ").is_visible(timeout=3000):
                    page.get_by_text("å†…å®¹ã«ã¤ã„ã¦ç¢ºèªã—ã¾ã—ãŸ").click()
                    page.get_by_role("button", name="æ¬¡ã¸").click()
                    page.wait_for_timeout(1000)
                    
                    # è™•ç† 2/2 å€åŸŸé¸æ“‡
                    page.get_by_label("ä¸–å¸¯(å€‹äºº)ã§").check()
                    page.locator("select").select_option(index=1)
                    page.get_by_role("button", name="ã‚µãƒ¼ãƒ“ã‚¹ã®åˆ©ç”¨ã‚’é–‹å§‹ã™ã‚‹").click()
                    print("âœ… æ“Šç©¿å‰å…©å±¤å°è¦½")
            except:
                print("â„¹ï¸ æœªåµæ¸¬åˆ°å‰å…©å±¤ï¼Œå¯èƒ½å·²è·³é")

            # 3. æ“Šç©¿ç¬¬ä¸‰å±¤ (Codegen éŒ„åˆ°çš„é‚£ä¸€æ­¥)
            try:
                # é€™è£¡ä½¿ç”¨ Codegen ç”¢ç”Ÿçš„ç²¾ç¢ºå®šä½
                target_btn = page.get_by_role("button", name="ç¢ºèªã—ã¾ã—ãŸ / I understand")
                target_btn.wait_for(state="visible", timeout=5000)
                target_btn.click()
                print("âœ… æˆåŠŸåŸ·è¡Œ Codegen éŒ„è£½çš„é»æ“Šï¼šç¢ºèªã—ã¾ã—ãŸ / I understand")
            except Exception as e:
                print(f"âš ï¸ ç„¡æ³•é»æ“Šç¬¬ä¸‰å±¤æŒ‰éˆ•: {e}")

            # 4. æœ€çµ‚ç¢ºèªï¼šå¦‚æœé®ç½©é‚„åœ¨ï¼Œæš´åŠ›ç§»é™¤ (ç¢ºä¿è¬ç„¡ä¸€å¤±)
            page.wait_for_timeout(2000)
            page.evaluate("""() => {
                document.querySelectorAll('div').forEach(div => {
                    const style = window.getComputedStyle(div);
                    if (style.position === 'fixed' && parseInt(style.zIndex) > 50) div.remove();
                });
                document.body.style.overflow = 'auto';
            }""")

            # æ“·å–å…§æ–‡
            html_content = page.content()
            browser.close()
            
            from bs4 import BeautifulSoup
            soup = BeautifulSoup(html_content, 'html.parser')
            # æŠ“å–æ–°ç‰ˆå…§æ–‡æ¨™ç±¤
            nodes = soup.find_all(['p', 'h3'], class_=['_1i1d7sh2', '_1i1d7sh9'])
            return [n.get_text().strip() for n in nodes if n.get_text().strip()]

    except Exception as e:
        print(f"âŒ åŸ·è¡Œå¤±æ•—: {e}")
        return []

# --- æ¸¬è©¦èˆ‡åŸ·è¡Œå€å¡Š ---
if __name__ == "__main__":
    print("ğŸ“¡ æ­£åœ¨å˜—è©¦æŠ“å– NHK æœ€æ–°æ–°è...")
    
    df = fetch_nhk_news()
    
    if not df.empty:
        os.makedirs('data', exist_ok=True)
        # å„²å­˜æ¸…å–®
        df.to_csv('data/latest_articles.csv', index=False, encoding='utf-8-sig')
        print(f"âœ… æˆåŠŸï¼æŠ“å–åˆ° {len(df)} å‰‡æ–°èæ¸…å–®ã€‚")
        
        # æ¸¬è©¦æŠ“å–ç¬¬ä¸€å‰‡çš„å…¨æ–‡
        first_url = df.iloc[0]['url']
        print(f"ğŸ” æ¸¬è©¦æŠ“å–ç¬¬ä¸€å‰‡å…¨æ–‡: {first_url}")
        content = fetch_article_full_text(first_url)
        print(f"ğŸ“ å…§æ–‡æ®µè½æ•¸: {len(content)}")
        for i, p in enumerate(content[:3]): # å°å‡ºå‰ä¸‰æ®µçœ‹çœ‹
            print(f"  æ®µè½ {i+1}: {p[:50]}...")
    else:
        print("âŒ ä¾ç„¶ç„¡æ³•æŠ“å–è³‡æ–™ï¼Œè«‹æª¢æŸ¥ JSON çµæ§‹ã€‚")